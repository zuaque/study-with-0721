# 差分隐私

差分隐私（Differential privacy）最早于2008年由Dwork 提出，通过严格的数学证明，使用随机应答（Randomized Response）方法确保数据集在输出信息时受单条记录的影响始终低于某个阈值，从而使第三方无法根据输出的变化判断单条记录的更改或增删，被认为是目前基于扰动的隐私保护方法中安全级别最高的方法。

##### 根据他们的定义，数据集中任何个人记录的存在或缺失都不应显著影响机制的结果。

##### 我们将任何可以对数据执行的计算称为“机制”。差分隐私处理的是随机化机制，这些分析的输出对于给定输入是概率性变化的。

##### 因此，如果对于只在一个记录上有所不同的任何两个数据集，任何结果发生的概率几乎相同，则认为机制是差分隐私的。

用于回答数值问题的最常见机制之一是**添加随机噪声**：在输出中添加足够的噪声来掩盖数据中任何可能的敏感信息，同时仍然保持分析的整体准确性。

#### 敏感度

##### 敏感度是指在两个数据集(彼此之间只相差一个元素)D和D'中，一个查询函数f(·)最大的变化范围，定义为：

![9223273cae9555f64e803888d7ecba2](.\9223273cae9555f64e803888d7ecba2.png)

#### KL-Divergence & MAX-Divergence

KL-Divergence又叫做相对熵，用于衡量两个概率分布之间的不同程度，离散随机变量相对熵的公式如下：![051c17562ecc5bbc69ba1eaa73c2457](.\051c17562ecc5bbc69ba1eaa73c2457.png)

但我们关注的并不是两个分布的整体差异，而只需要求出两个分布之间的最大差异，因此引入了MAX-Divergence，并且需要它小于ε：![a688f986f835a23b9138501ae806e95](.\a688f986f835a23b9138501ae806e95.png)

将不等式进行化简，用e指数运算将ln符号消去，并移动分母，将离散型变量替换成查询结果M,最终得到式子：![a2c029c2580bf0b60d656fb3cfc1036](.\a2c029c2580bf0b60d656fb3cfc1036.png)

其中M(x) = f(x)  + r， 代表查询函数，r代表随机噪声，M(x)代表最终的查询结果，S代表该查询结果的可能取值范围。![617fdd7718ca20b1913ff7e824b02d4](.\617fdd7718ca20b1913ff7e824b02d4.png)

#### 松弛

在拉普拉斯机制的定义中，差分隐私过于严格导致了可用性的下降，而实际应用中需要更多的隐私预算，因此为了算法的实用性，后续又引入了松弛版本的差分隐私，即：![09c065aba2388d1e71fefa15038be72](.\09c065aba2388d1e71fefa15038be72.png)

 其中δ作为松弛项，代表一个可以容忍的较小差距，一般设置得比较小。

#### 拉普拉斯机制

拉普拉斯分布![8b1f6687946db56e493737c3daf7fa9](.\8b1f6687946db56e493737c3daf7fa9.png)

该分布叫做拉普拉斯分布，其中μ代表位置参数，b代表尺度参数。记作Laplace(μ,b)

![94ed1c59d1d24ff3757b59652dfa577](.\94ed1c59d1d24ff3757b59652dfa577.png)

当b=1时，拉普拉斯分布与标准正态分布的对比

相比于标准正态分布，服从拉普拉斯分布的随机变量，出现极端大的值的概率要远远大于正态分布(长尾分布性质)。因此它能够更好地描述极端事件的发生情况，精度更高。

